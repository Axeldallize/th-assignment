# AI Data Analyst for Pagila Database

This project provides a smart, conversational API that allows users to ask analytical questions about a PostgreSQL database (using the Pagila sample dataset) in plain English and receive accurate answers.

---

## Architecture

This project uses a **Tool-Augmented LLM** architecture. Instead of using separate, disconnected AI calls for different tasks, this approach uses a single, stateful conversation with an AI model to orchestrate the entire process of answering a user's question.

The core workflow is as follows:

1.  **Understand & Generate SQL:** The user's question and the database schema are given to the AI. Its first task is to generate the correct PostgreSQL query needed to fetch the relevant data.

2.  **Execute & Self-Correct:** The application executes the generated SQL. If the database returns an error (e.g., due to incorrect SQL syntax), the error is fed *back* to the AI in the same conversation. The AI is then prompted to fix its own mistake and generate a new query. This "self-correction" loop makes the system highly robust.

3.  **Analyze & Answer:** Once the SQL executes successfully, the resulting data is given back to the AI. Its final task is to analyze this data in the context of the user's original question and formulate a clear, human-readable answer.

This single-conversation approach was chosen because it's efficient, robust, and allows the AI to maintain context throughout the entire process, leading to more coherent and accurate results.

---

## Security

Security is a critical consideration when allowing an AI to interact with a database. This project implements three key layers of defense:

1.  **Read-Only Database User:** The application connects to the PostgreSQL database using a dedicated user with read-only (`SELECT`) permissions. This is the most important security measure, as it fundamentally prevents the AI from performing any destructive operations (like `DROP`, `UPDATE`, or `DELETE`).

2.  **Query Validation:** Before execution, every SQL query generated by the AI is validated to ensure it is a `SELECT` statement and does not contain any other forbidden keywords. This acts as a secondary line of defense.

3.  **Statement Timeout:** A timeout (e.g., 5 seconds) is enforced on every query sent to the database. This prevents long-running or malicious queries (e.g., a Cartesian join that never finishes) from bogging down the system.

---

## How to Run

Follow these steps to get the project running locally.

1.  **Clone the Repository:**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-name>
    ```

2.  **Set Up Environment Variables:**
    Create a `.env` file from the example and fill in your details.
    ```bash
    cp .env.example .env
    ```
    You will need to add your PostgreSQL read-only user credentials and your LLM API key.

3.  **Prepare the Database Scripts:**
    This project uses the Pagila sample database. The following commands will download the necessary schema and data files into the `init-db` directory. The `docker-compose` setup will automatically run these scripts the first time it creates the database.
    ```bash
    # Create the directory if it doesn't exist
    mkdir -p init-db
    # Download the schema and data files
    curl -o init-db/1-pagila-schema.sql https://raw.githubusercontent.com/devrimgunduz/pagila/master/pagila-schema.sql
    curl -o init-db/2-pagila-data.sql https://raw.githubusercontent.com/devrimgunduz/pagila/master/pagila-data.sql
    ```

4.  **Start the Database:**
    Make sure you have Docker installed and running. This command will start the database server and automatically populate it using the scripts from the previous step.
    ```bash
    docker-compose up -d
    ```
    *(Note: If you need to reset the database, run `docker-compose down -v` to remove the old data volume before running `up -d` again.)*

5.  **Install Dependencies:**
    First, create and activate a virtual environment:
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate
    ```
    Now, install the required packages:
    ```bash
    pip install -r requirements.txt
    ```

6.  **Run the API Server:**
    With the virtual environment active, run the server:
    ```bash
    uvicorn app:app --reload
    ```
    The server will be available at `http://127.0.0.1:8000`.

### API Usage Examples

You can interact with the API via the `/docs` endpoint or using `curl`.

**Sample Question 1: Simple Retrieval**
```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "What actors were in Chocolat Harry?"
}'
```

**Sample Question 2: Aggregation & Ranking**
```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "Display the top 3 actors who have most appeared in films in the Children category"
}'
```

**Sample Question 3: Analytical**
```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "A common criticism of modern movies is that they are too long. Can you analyze film lengths over time and determine if that criticism is fair"
}'
```

**Sample Question 4: Min/Max**
```bash
curl -X 'POST' \
  'http://127.0.0.1:8000/query' \
  -H 'Content-Type: application/json' \
  -d '{
  "question": "Which customer has paid the most for rentals? What about least?"
}'
```

## Sample Questions

### Simple Data Retrieval
```bash
# Q1: What actors were in Chocolat Harry?
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "What actors were in Chocolat Harry?"}'

# Q2: Display the top 3 actors who have most appeared in films in the Children category
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "Display the top 3 actors who have most appeared in films in the Children category"}'

# Q4: Which customer has paid the most for rentals? What about least?
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "Which customer has paid the most for rentals? What about least?"}'
```

### Analytical Questions
```bash
# Q3: Analyze film lengths over time
curl -X POST "http://localhost:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "A common criticism of modern movies is that they are too long. Can you analyze film lengths over time and determine if that criticism is fair"}'
```

## API Documentation

### Endpoints

- `GET /` - API information
- `POST /query` - Submit natural language questions
- `GET /health` - Health check

### Request Format

```json
{
  "question": "Your natural language question about the database"
}
```

### Response Format

**Data Retrieval Response:**
```json
{
  "answer_type": "data",
  "content": [
    {"column1": "value1", "column2": "value2"}
  ],
  "sql_query": "SELECT ..."
}
```

**Analytical Response:**
```json
{
  "answer_type": "analysis", 
  "content": "Detailed analysis with insights...",
  "sql_query": "SELECT ..."
}
```

## Database Schema

The system uses the Pagila database (PostgreSQL sample database) with tables including:
- `actor`, `film`, `category` - Core content
- `customer`, `rental`, `payment` - Business transactions  
- `film_actor`, `film_category` - Relationships

## Configuration

### Environment Variables

```env
# Required
ANTHROPIC_API_KEY=your-key-here

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/pagila

# Optional
MODEL_NAME=claude-3-5-sonnet-20241022
LOG_LEVEL=INFO
PORT=8000
```

## Development

### Project Structure

```
├── app.py              # FastAPI application
├── config.py           # Configuration management  
├── database.py         # Database utilities
├── llm_client.py       # Anthropic API client
├── prompt_templates.py # LLM prompt templates
├── docker-compose.yml  # Database setup
└── requirements.txt    # Dependencies
```

### Testing

```bash
# Run basic tests
python -c "from database import DatabaseManager; db = DatabaseManager(); print('✅ Database connection works')"

# Test LLM integration
python -c "from llm_client import AnthropicClient; client = AnthropicClient(); print('✅ LLM client works')"
```

## Limitations and Future Work

### Current Limitations
- **SQL Accuracy**: LLM may generate incorrect queries for complex joins
- **Schema Scalability**: Full schema included in prompts (not scalable)
- **Basic Validation**: Simple query validation, not foolproof

### Production Improvements
- **RAG-Based Schema**: Retrieve only relevant tables for large databases
- **Advanced Query Validation**: AST parsing for comprehensive safety
- **Query Optimization**: Analyze and improve generated SQL performance
- **Caching Layer**: Cache results for common queries
- **User Management**: Authentication and query logging

## Troubleshooting

### Common Issues

**Database Connection Error:**
```bash
# Check if database is running
docker-compose ps
docker-compose logs postgres
```

**API Key Error:**
```bash
# Verify API key is set
echo $ANTHROPIC_API_KEY
```

**SQL Execution Error:**
- Check generated SQL in response
- Verify table names and relationships
- Review application logs

## License

This project is for educational/interview purposes.
